# Main
train.Sender = @UniformBias
train.Recver = @DeterministicGradient
train.env = @IteratedSenderRecver()
train.episodes = 15000
train.render_freq = 1000
train.log_freq = 100
train.savedir = None
train.device = "cpu"

# Game
IteratedSenderRecver.batch_size = 128
IteratedSenderRecver.num_rounds = 10
IteratedSenderRecver.num_targets = 100
IteratedSenderRecver.max_bias = 10
IteratedSenderRecver.min_bias = 0

# Utils
ReplayBuffer.capacity = 256000

# Agents
UniformBias.bias = 0

DeterministicGradient.lr = 1e-4

#NaiveQNet.n = %n
#NaiveQNet.gamma = 0.96
#NaiveQNet.alpha = 1e-2
#NaiveQNet.decay = 1e-4
#NaiveQNet.epsilon = 0.05

#OneShotQNet.n = %n
#OneShotQNet.gamma = 0.96
#OneShotQNet.alpha = 1e-2
#OneShotQNet.decay = 1e-4
#OneShotQNet.epsilon = 0.05

#PolicyGradient.lr = 1e-3
#PolicyGradient.weight_decay = 0 #1e-2
#PolicyGradient.gamma = 1.00
#PolicyGradient.ent_reg = 1e-4
#PolicyGradient.min_std = 1.

#A2C.lr = 1e-3
#A2C.gamma = 1.00
#A2C.ent_reg = 1e-4
#A2C.min_std = 1.

DDPG.actor_lr = 1e-4
DDPG.critic_lr = 5e-4
DDPG.gamma = 0.99
DDPG.tau = 0.001
DDPG.batch_size = 128
DDPG.warmup_episodes = 100

#MADDPG.num_agents = 2
#MADDPG.lr = 1e-4
#MADDPG.gamma = 0.99
#MADDPG.tau = 0.01
#MADDPG.batch_size = 128
#MADDPG.state_dim = 7
